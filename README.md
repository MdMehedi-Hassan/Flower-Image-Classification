# ğŸŒ¸ Flower Classification â€“ Deep Learning (CNN) vs Classical ML (HOG + Boosting)

This project explores **two approaches** for classifying flower images into **5 categories**:

1. **Deep Learning (CNN with TensorFlow/Keras)** â€“ learns features directly from raw pixel data.  
2. **Classical Machine Learning (HOG + Boosting Classifiers)** â€“ uses handcrafted **Histogram of Oriented Gradients (HOG)** features with **CatBoost, LightGBM, and XGBoost**.

The comparison highlights the strengths of deep learning over traditional ML in image classification tasks.

---

## ğŸ“‚ Dataset
- Source: Custom flower dataset ([Google Drive, ~145 MB](https://drive.google.com/file/d/1nzf5b_NNJlnInkueIWCOG7PWFv-Zk2Wk/view?usp=drive_link)).  
- Classes:
  - ğŸŒº Astilbe  
  - ğŸŒ» Sunflower  
  - ğŸŒ¼ Dandelion  
  - ğŸŒ¸ Magnolia  
  - ğŸŒ¹ Rose  

**Dataset Split:**
- 70% Training  
- 20% Validation  
- 10% Testing  


---

## ğŸ—ï¸ Approach 1 â€“ Convolutional Neural Network (CNN)

### ğŸ”¹ Model Architecture
- Conv2D â†’ MaxPooling â†’ Dropout  
- Conv2D â†’ MaxPooling â†’ Dropout  
- Flatten â†’ Dense(256) â†’ Dropout  
- Dense(5, softmax)

### ğŸ”¹ Training Details
- **Image Size:** 256 Ã— 256 Ã— 3  
- **Optimizer:** Adam  
- **Loss Function:** Sparse Categorical Crossentropy  
- **Batch Size:** 32  
- **Epochs:** 50  
- **Data Augmentation:** Random flip, rotation, zoom, contrast  

### ğŸ”¹ Results
- âœ… Training Accuracy: ~97%  
- âœ… Validation Accuracy: ~92â€“93%  
- âœ… **Test Accuracy: 90.25%**  

---

## ğŸ—ï¸ Approach 2 â€“ Classical ML (HOG + Boosting)

### ğŸ”¹ Feature Extraction
- Used **HOG (Histogram of Oriented Gradients)** to extract shape & edge features.  
- Each image â†’ ~34,596 HOG features.  
- Features normalized using `MinMaxScaler`.  

### ğŸ”¹ Models Trained
- **CatBoostClassifier**  
- **LightGBMClassifier**  
- **XGBoostClassifier**

### ğŸ”¹ Results
| Model                | Accuracy | Precision | Recall | F1   | Balanced Acc. |
|-----------------------|----------|-----------|--------|------|---------------|
| **CNN (TensorFlow)** | **90.25%** | 0.91      | 0.90   | 0.90 | 0.89          |
| CatBoost             | 68.3%    | 0.69      | 0.68   | 0.68 | 0.65          |
| LightGBM             | 65.4%    | 0.67      | 0.65   | 0.65 | 0.61          |
| XGBoost              | 64.2%    | 0.65      | 0.64   | 0.63 | 0.60          |

âœ… CNN significantly outperforms boosting classifiers.  
âš¡ CatBoost was the best among classical ML models.

---

## ğŸ“ˆ Visualizations
- Training vs Validation Accuracy curves (CNN).  
- Confusion Matrices for CNN, CatBoost, LightGBM, and XGBoost.  
- Bar chart comparison of Accuracy, Precision, Recall, and F1 scores.  

---

## ğŸš€ How to Run

### 1ï¸âƒ£ Clone the repository
```bash
git clone https://github.com/your-username/flower-classification.git
cd flower-classification
